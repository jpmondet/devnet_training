# Information theory

## Mathematical model for Communication

## Entropy

Assuming a system with only 1 source, 1 receiver and 1 channel, a model can
define the state of knowledge of a receiver. When the receiver receive
a message over the channel, its knowledge increases by the **information content** of the message.

The model implies being able measure the receiver's knowledge. 

In a situation where the source have to send 1 of N equiprobable messages, if
the receiver knows it will receive 1 of N message but doesn't know which one,
we call the **entropy** (H) the initial degree of uncertainty of the receiver : H = log N (the log base 2 comes from the fact that it can be modelized as a binary tree)

However, if the messages are not equiprobable, the entropy becomes dependent of the probabilities of each message.

This can be modelized by defining a random variable (X) corresponding to a chosen message and having discrete values x1, x2, ... xN such that P{X=xi} = P(xi) and then taking the entropy of this random variable : H(X) = -SUMi(P(xi).log P(xi))

Note that entropies of independent random variables are additive : H(XY) = H(X) + H(Y)


/!\ This is assuming that messages to be sent are finite and we know the probability to select one message or the other. This limitation is addressed by :
 - representing the infinite set of messages as **symbols**
 - computing entropy at the symbol level (**symbol entropy**)
 - identifying the underlying alphabet of symbols and computing the relative frequencies of occurrence of longer and longer symbol sequences in a representative message corpus we can approximately determine its entropy.

## Encoding

To shorten the messages as much as possible, the symbols are encoded. But from this, arise the problem of finding the **optimal encoding**. 

A code should be **unambiguous** to avoid confusing the receiver. Also it should (but nowadays, it is not really mandatory/true) be an **instantaneous code** (code that can be decoded immediately -> no codeword should be the prefix of another one).

But to be optimal, instantaneous is not enough, the code must also : 
- construct message symbols from longer sequences of elementary symbols (which makes it more efficient)
- take probabilities into account by assigning shorter codes to frequent symbols

**Kraft inequality** : SUMi(2^(-li)) <= 1  (for instantenous code and with li being the level in the binary tree of the code).  It constrains the choice of codeword lengths

It permits to show that the expected length of a codeword is lower-bounded by entropy : E >= H(X)

The **Huffman code** approaches H. It even guarantees to have an average
codeword no longer than H+1. It is found by assigning symbols to leaf nodes of a tree sorted by probabilities of those symbols (least-likely to most-likely).

## Modelization of a Source

- Message source is seen as a random variable X = {x1, x2, ...xn} 
- Values of the RV being messages composed of 1 or more symbols 
- Messages are represented by binary code
- Expected codeword length is at least H(X) bits
- Message source's **entropy rate** is at least MH(X)/sec (M = nb of messages/sec)
- The entropy rate is also called the **information rate**.
- The source can generate up to 2^(MH(X)) distinct messages/sec
- information rate <= log(nb of distinct messages/sec)

**Asymptotic Equipartioning Property** : The messages generated by a Message source (that selects messages symbols i.i.d) are most likely elements from the **typical set** with is the set of 2^(H(X)) elements. The other messages are from the **atypical** set. The atypical set can be ignored so the messages can be coded with only H(X) bits. The rest being seen as errors. 

i.i.d = idependently and with identical distribution

## Modelization of a Channel

[communication channel](./random_web_findings/com-chan.png)

Previous sections spoke about message source & source coder.

Here comes the channel part.

It is inefficient to transfer message symbols on a channel, this is why there is a channel coder that translate message symbols to **channel symbols**. This channel coder/decoder prevents being affected by noise (or at least minimize the issues). 

If channel symbols are only 0s and 1s, a **binary channel** is used. To increase **information capacity**, complex symbols can be used to transport more bit in parallel (like QAM modulation on fibers today)

### Capacity of Noiseless channel

If 
- 2^k distinct channel symbols
- C channel symbols per seconf
Then information capacity of kC bits/second

**(FUNDAMENTAL) Channel Capacity Theorem** : Noiseless channel of capacity kC can carry messages from a source that generates entropy at the rate of MH(X) bits per second if and only if : **kC > MH(X)**

### Noisy Channel

Capacity is reduced due to errors. The reduction depends on the channel, the message source and the channel coder.

Errors implie receiving a '0' instead of a '1' (or 1->0). This is taken into account by considering the **joint probability distribution** of the channel symbol send and the channel symbol received.

This consideration exhibit 2 effects of a noisy channel : 
- Given a distribution of symbol probabilities P(X) at the output of the channel coder, the noisy channel modifies the symbol probability distribution at the input of channel decoder to P(Y).
- The channel decoder must work backwards to determine which symbol had been sent when a particular symbol is received

This also generate a whole lot of new entropies :
- H(X) that corresponds to the uncertainty at the channel decoder (which symbol was sent by the channel coder?)
- H(Y) : uncertainty at the channel coder (which symbol will be received at the channel decoder?)
- H(XY) : uncertainty of the occurrence of the symbol pair (transmitted/received) at both the channel coder & decoder
- H(X|Y) : uncertainty at the channel decoder again but considering that the symbol Y is received. (variants : H(X|Y=0 or 1))
- H(Y|X) : uncertainty at the channel coder again but considering that the symbol X is transmitted.

Related to those entropies is the **mutual information** between a sender and a receiver that is defined to be the reduction in the uncertainty :
- of the receiver about the sembol sent when receiving a symbol -> I(X;Y) = H(X) - H(X|Y)
- of the sender about the symbol received at the receiver due to the transmission of the symbol -> I(X;Y) = H(Y) - H(Y|X)
More intuitively, it can be seen as the actual information content carried on a noisy channel by symbols (capacity of a noisy channel). For example, a very noisy channel could carry only 0.01bits/symbol of worthy information. The rest is wasted.

Hence, the coding scheme must aim to maximize I(X;Y). Moreover, Shannon showed that this max is also the greatest rate at which information can be carried over a noisy channel without introducing uncorrectable errors (**turbo codes** or **Low Density Parity codes** approach this max rate).

Shannon's theorem also show : 
- that sequences of i.i.d typical symbols maps to a set of 2^(nH(Y|X)) non-overlapping sequences of symbols that are also typical
- that the sender can send a maximum of 2^(n(H(Y)) - H(Y|X)) distinct decodable messages.
- The contribution of each symbol to the total set of messages is bounded by 2^(I(X;Y)).

### The Gaussian Channel

This approach aims to facilitate the determination of the capacity of a channel and to modelized the fact that real life channels are continuous.

The huge takeaway is that channel capacity can be pretty easily found by computing : **Channel Capacity = bandwidth * log (1+P/N)**. The bandwidth is the width of the continuous channel over which is it carried (in hertz). The second part (log) is usually denoted as the SNR(dB) = 10log10(P/N).

However, in some specific use cases (like WiFi) interferences made by multiple senders diminish greatly the Channel Capacity.




















  




